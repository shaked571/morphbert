{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current setup using this notebook\n",
    "<p>label - POS</p>\n",
    "<p>Fine tune on: raw-train</p>\n",
    "<p>Evaluate on: Raw-dev (and Raw-test, but not reported)</p>\n",
    "<p>Classification by: whole word (as opposed to prefix/host)</p>\n",
    "<p>Morphologically informed labels? None </p>\n",
    "<p>Shuffle/Sort? shuffle </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import bclm\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually setting seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "np.random.seed(3)\n",
    "torch.cuda.manual_seed_all(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "`bclm.read_dataframe('spmrl', subset='train')` - gives the gold-segmented tokens <br>\n",
    "`bclm.get_token_df(train, ['upostag'])` - gives the raw tokens<br>\n",
    "`bclm.read_dataframe('yap_dev')` - gives the YAP tokenization (only available on `yap_dev` and `yap_test`. No `yap_train`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bclm.read_dataframe('spmrl', subset='train')\n",
    "train_df = bclm.get_token_df(train, ['upostag'])\n",
    "train_df['token_str'] = train_df['token_str'].str.replace('”','\"')\n",
    "\n",
    "dev = bclm.read_dataframe('spmrl', subset='dev')\n",
    "dev_df = bclm.get_token_df(dev, ['upostag'])\n",
    "dev_df['token_str'] = dev_df['token_str'].str.replace('”','\"')\n",
    "\n",
    "test = bclm.read_dataframe('spmrl', subset='test')\n",
    "test_df = bclm.get_token_df(test, ['upostag'])\n",
    "test_df['token_str'] = test_df['token_str'].str.replace('”','\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>token_str</th>\n",
       "      <th>upostag</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>עשרות</td>\n",
       "      <td>CDT</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>אנשים</td>\n",
       "      <td>NN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>מגיעים</td>\n",
       "      <td>BN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>מתאילנד</td>\n",
       "      <td>PREPOSITION^NNP</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>לישראל</td>\n",
       "      <td>PREPOSITION^NNP</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>כשהם</td>\n",
       "      <td>TEMP^PRP</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>נרשמים</td>\n",
       "      <td>BN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>כמתנדבים</td>\n",
       "      <td>PREPOSITION^NN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>,</td>\n",
       "      <td>yyCM</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>אך</td>\n",
       "      <td>CC</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>למעשה</td>\n",
       "      <td>RB</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>משמשים</td>\n",
       "      <td>BN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>עובדים</td>\n",
       "      <td>NN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>שכירים</td>\n",
       "      <td>JJ</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>זולים</td>\n",
       "      <td>JJ</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>.</td>\n",
       "      <td>yyDOT</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>תופעה</td>\n",
       "      <td>NN</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>זו</td>\n",
       "      <td>PRP</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>התבררה</td>\n",
       "      <td>VB</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>אתמול</td>\n",
       "      <td>RB</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id  token_id token_str          upostag  set\n",
       "0         1         1     עשרות              CDT  dev\n",
       "1         1         2     אנשים               NN  dev\n",
       "2         1         3    מגיעים               BN  dev\n",
       "3         1         4   מתאילנד  PREPOSITION^NNP  dev\n",
       "4         1         5    לישראל  PREPOSITION^NNP  dev\n",
       "5         1         6      כשהם         TEMP^PRP  dev\n",
       "6         1         7    נרשמים               BN  dev\n",
       "7         1         8  כמתנדבים   PREPOSITION^NN  dev\n",
       "8         1         9         ,             yyCM  dev\n",
       "9         1        10        אך               CC  dev\n",
       "10        1        11     למעשה               RB  dev\n",
       "11        1        12    משמשים               BN  dev\n",
       "12        1        13    עובדים               NN  dev\n",
       "13        1        14    שכירים               JJ  dev\n",
       "14        1        15     זולים               JJ  dev\n",
       "15        1        16         .            yyDOT  dev\n",
       "16        2         1     תופעה               NN  dev\n",
       "17        2         2        זו              PRP  dev\n",
       "18        2         3    התבררה               VB  dev\n",
       "19        2         4     אתמול               RB  dev"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform column names\n",
    "Note that the column names in yap dfs can be slightly different from spmrl dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Evaluating on Raw-dev/Raw-test\n",
    "train_df.rename(columns = {\"token_str\": \"form\"}, inplace = True)\n",
    "dev_df.rename(columns = {\"token_str\": \"form\"}, inplace = True)\n",
    "test_df.rename(columns = {\"token_str\": \"form\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Evaluating on Yap-dev/Yap-test\n",
    "# dev_df.rename(columns = {\"misc_token_id\": \"token_id\"}, inplace = True)\n",
    "# test_df.rename(columns = {\"misc_token_id\": \"token_id\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dev_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Add sorting on the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sent_len'] = train_df.groupby('sent_id').id.transform('size')\n",
    "s = train_df.sort_values(by=['sent_len', 'sent_id', 'id']).index\n",
    "train_df_sorted = train_df.reindex(s)\n",
    "train_df_sorted.groupby('sent_id', sort=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_sorted.iloc[150:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lists of sentences\n",
    "Note that longest sentences from dev and test splits need to be removed, and those sentences change depending on wether or not sorting was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הם', 'התבקשו', 'לדווח', 'למשטרה', 'על', 'תנועותיהם', '.']\n",
      "['PRP', 'VB', 'VB', 'PREPOSITION^DEF^NN', 'IN', 'NN', 'yyDOT']\n",
      "490\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "class sentenceGetter(object):\n",
    "    def __init__(self, data, max_sent=None):\n",
    "        self.index = 0\n",
    "        self.max_sent = max_sent\n",
    "        self.tokens = data['form']\n",
    "        self.labels = data['upostag']\n",
    "        #for evaluating by word-accuracy\n",
    "        self.correspondingToken = data['token_id']\n",
    "        self.orig_sent_id = data['sent_id']\n",
    "    \n",
    "    def sentences(self):\n",
    "        sent = []\n",
    "        counter = 0\n",
    "        \n",
    "        for token,label, corres_tok, sent_id in zip(self.tokens, self.labels, self.correspondingToken, self.orig_sent_id):\n",
    "            sent.append((token, label, corres_tok, sent_id))\n",
    "            if token.strip() == \".\":\n",
    "                yield sent\n",
    "                sent = []\n",
    "                counter += 1\n",
    "            if self.max_sent is not None and counter >= self.max_sent:\n",
    "                return\n",
    "\n",
    "train_getter = sentenceGetter(train_df)\n",
    "dev_getter = sentenceGetter(dev_df)\n",
    "test_getter = sentenceGetter(test_df)\n",
    "\n",
    "train_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
    "train_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in train_getter.sentences()]\n",
    "\n",
    "dev_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_corresTokens = [[corres_tok for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "dev_sent_ids = [[sent_id for token, label, corres_tok, sent_id in sent] for sent in dev_getter.sentences()]\n",
    "\n",
    "test_sentences = [[token for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "test_labels = [[label for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "test_corresTokens = [[corres_tok for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "test_sent_ids = [[sent_id for token, label, corres_tok, sent_id in sent] for sent in test_getter.sentences()]\n",
    "\n",
    "print(train_sentences[10])\n",
    "print(train_labels[10])\n",
    "\n",
    "print(len(dev_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the longest sentences in the dev and test sets\n",
    "longest_sent_len = 0\n",
    "for sent in dev_sentences:\n",
    "    if len(sent) >= longest_sent_len:\n",
    "        print(len(sent))\n",
    "        longest_sent_len = len(sent)\n",
    "        print(\"index of longest sentence:{} \".format(dev_sentences.index(sent)))\n",
    "        \n",
    "longest_sent_len = 0\n",
    "for sent in test_sentences:\n",
    "    if len(sent) >= longest_sent_len:\n",
    "        print(len(sent))\n",
    "        longest_sent_len = len(sent)\n",
    "        print(\"index of longest sentence:{} \".format(test_sentences.index(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove too long sentences\n",
    "\n",
    "del dev_sentences[296]\n",
    "del dev_labels[296]\n",
    "del dev_corresTokens[296]\n",
    "del dev_sent_ids[296]\n",
    "\n",
    "del dev_sentences[226]\n",
    "del dev_labels[226]\n",
    "del dev_corresTokens[226]\n",
    "del dev_sent_ids[226]\n",
    "\n",
    "del dev_sentences[57]\n",
    "del dev_labels[57]\n",
    "del dev_corresTokens[57]\n",
    "del dev_sent_ids[57]\n",
    "\n",
    "del dev_sentences[49]\n",
    "del dev_labels[49]\n",
    "del dev_corresTokens[49]\n",
    "del dev_sent_ids[49]\n",
    "\n",
    "\n",
    "del test_sentences[396]\n",
    "del test_labels[396]\n",
    "del test_corresTokens[396]\n",
    "del test_sent_ids[396]\n",
    "\n",
    "del test_sentences[164]\n",
    "del test_labels[164]\n",
    "del test_corresTokens[164]\n",
    "del test_sent_ids[164]\n",
    "\n",
    "del test_sentences[157]\n",
    "del test_labels[157]\n",
    "del test_corresTokens[157]\n",
    "del test_sent_ids[157]\n",
    "\n",
    "del test_sentences[151]\n",
    "del test_labels[151]\n",
    "del test_corresTokens[151]\n",
    "del test_sent_ids[151]\n",
    "\n",
    "# ## YAP deletions\n",
    "# del dev_sentences[296]\n",
    "# del dev_labels[296]\n",
    "# del dev_corresTokens[296]\n",
    "# del dev_sent_ids[296]\n",
    "\n",
    "# del dev_sentences[226]\n",
    "# del dev_labels[226]\n",
    "# del dev_corresTokens[226]\n",
    "# del dev_sent_ids[226]\n",
    "\n",
    "# del dev_sentences[57]\n",
    "# del dev_labels[57]\n",
    "# del dev_corresTokens[57]\n",
    "# del dev_sent_ids[57]\n",
    "\n",
    "# del dev_sentences[49]\n",
    "# del dev_labels[49]\n",
    "# del dev_corresTokens[49]\n",
    "# del dev_sent_ids[49]\n",
    "\n",
    "# del dev_sentences[24]\n",
    "# del dev_labels[24]\n",
    "# del dev_corresTokens[24]\n",
    "# del dev_sent_ids[24]\n",
    "\n",
    "# del dev_sentences[22]\n",
    "# del dev_labels[22]\n",
    "# del dev_corresTokens[22]\n",
    "# del dev_sent_ids[22]\n",
    "\n",
    "# del dev_sentences[12]\n",
    "# del dev_labels[12]\n",
    "# del dev_corresTokens[12]\n",
    "# del dev_sent_ids[12]\n",
    "\n",
    "# del dev_sentences[9]\n",
    "# del dev_labels[9]\n",
    "# del dev_corresTokens[9]\n",
    "# del dev_sent_ids[9]\n",
    "\n",
    "# del dev_sentences[5]\n",
    "# del dev_labels[5]\n",
    "# del dev_corresTokens[5]\n",
    "# del dev_sent_ids[5]\n",
    "\n",
    "# del test_sentences[386]\n",
    "# del test_labels[386]\n",
    "# del test_corresTokens[386]\n",
    "# del test_sent_ids[386]\n",
    "\n",
    "# del test_sentences[384]\n",
    "# del test_labels[384]\n",
    "# del test_corresTokens[384]\n",
    "# del test_sent_ids[384]\n",
    "\n",
    "# del test_sentences[377]\n",
    "# del test_labels[377]\n",
    "# del test_corresTokens[377]\n",
    "# del test_sent_ids[377]\n",
    "\n",
    "# del test_sentences[213]\n",
    "# del test_labels[213]\n",
    "# del test_corresTokens[213]\n",
    "# del test_sent_ids[213]\n",
    "\n",
    "# del test_sentences[141]\n",
    "# del test_labels[141]\n",
    "# del test_corresTokens[141]\n",
    "# del test_sent_ids[141]\n",
    "\n",
    "# del test_sentences[124]\n",
    "# del test_labels[124]\n",
    "# del test_corresTokens[124]\n",
    "# del test_sent_ids[124]\n",
    "\n",
    "# del test_sentences[45]\n",
    "# del test_labels[45]\n",
    "# del test_corresTokens[45]\n",
    "# del test_sent_ids[45]\n",
    "\n",
    "# del test_sentences[35]\n",
    "# del test_labels[35]\n",
    "# del test_corresTokens[35]\n",
    "# del test_sent_ids[35]\n",
    "\n",
    "# del test_sentences[23]\n",
    "# del test_labels[23]\n",
    "# del test_corresTokens[23]\n",
    "# del test_sent_ids[23]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Number of gpus: 4\n",
      "Name of gpu: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print(\"Device: \" + str(device))\n",
    "print(\"Number of gpus: \" + str(n_gpu))\n",
    "print(\"Name of gpu: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['הם', 'ה', '##ת', '##בק', '##שו', 'ל', '##דו', '##וח', 'ל', '##משטרה', 'על', 'ת', '##נוע', '##ות', '##יהם', '.']\n",
      "['PRP', 'VB', 'VB', 'VB', 'VB', 'VB', 'VB', 'VB', 'PREPOSITION^DEF^NN', 'PREPOSITION^DEF^NN', 'IN', 'NN', 'NN', 'NN', 'NN', 'yyDOT']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "def tokenize(sentences, orig_labels):\n",
    "    tokenized_texts = []\n",
    "    labels = []\n",
    "    for sent, sent_labels in zip(sentences, orig_labels):\n",
    "        bert_tokens = []\n",
    "        bert_labels = []\n",
    "        for orig_token, orig_label in zip(sent, sent_labels):\n",
    "            b_tokens = tokenizer.tokenize(orig_token)\n",
    "            bert_tokens.extend(b_tokens)\n",
    "            for b_token in b_tokens:\n",
    "                bert_labels.append(orig_label)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "        labels.append(bert_labels)\n",
    "        assert len(bert_tokens) == len(bert_labels)\n",
    "    return tokenized_texts, labels\n",
    "\n",
    "train_tokenized_texts, train_tokenized_labels = tokenize(train_sentences, train_labels)\n",
    "print(train_tokenized_texts[10])\n",
    "print(train_tokenized_labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAD': 0, 'REL^PREPOSITION^CDT': 1, 'DEF^DTT': 2, 'CONJ^VB^AT^PRP': 3, 'CONJ^yyQUOT^NNT': 4, 'PREPOSITION^ADVERB^CD': 5, 'TEMP^PRP': 6, 'CONJ^TEMP^RB': 7, 'AT': 8, 'CONJ^REL^COP': 9, 'PREPOSITION^ADVERB^NCD': 10, 'PREPOSITION^ADVERB^CDT': 11, 'CONJ^DTT': 12, 'TEMP^PREPOSITION^DEF^NN': 13, 'TEMP^NNP': 14, 'IN^IN^NNT': 15, 'DEF^P': 16, 'ADVERB^DTT': 17, 'VB^AT^S_ANP': 18, 'CONJ^DEF^NNP': 19, 'CD': 20, 'PREPOSITION^PREPOSITION^DEF^PRP': 21, 'PREPOSITION^yyQUOT^DEF^NN': 22, 'REL^DTT': 23, 'CONJ^IN': 24, 'POS': 25, 'PREPOSITION^JJ': 26, 'TEMP^NNT': 27, 'REL^yyQUOT^NNP': 28, 'CONJ^NNP': 29, 'IN^NN': 30, 'PREPOSITION^CDT': 31, 'PREPOSITION^NNP': 32, 'CONJ^CC': 33, 'yyLRB': 34, 'DEF^NN': 35, 'BNT': 36, 'REL^DEF^BN': 37, 'REL^yyQUOT^JJ': 38, 'PREPOSITION^DEF^yyQUOT^NNP': 39, 'REL': 40, 'NN': 41, 'IN^NNT': 42, 'TEMP^RB': 43, 'DEF^RB': 44, 'REL^JJ': 45, 'IN': 46, 'JJ': 47, 'PREPOSITION^DEF^CD': 48, 'CONJ^REL^PREPOSITION^NN': 49, 'CONJ^yyQUOT^NN': 50, 'CONJ^PREPOSITION^CDT': 51, 'CONJ^MD': 52, 'IN^PRP': 53, 'CONJ^yyQUOT^DEF^JJ': 54, 'PREPOSITION^POS^S_PRN': 55, 'PREPOSITION^CC': 56, 'CONJ^INTJ': 57, 'CONJ^PRP': 58, 'CONJ^REL^BN': 59, 'PREPOSITION^JJT': 60, 'REL^PREPOSITION^DTT': 61, 'PREPOSITION^DEF^BN': 62, 'IN^VB': 63, 'PREPOSITION^DEF': 64, 'REL^DT': 65, 'CONJ^yyQUOT^IN': 66, 'TEMP^VB': 67, 'CONJ^DEF^JJ': 68, 'CONJ^IN^DEF^NN': 69, 'PREPOSITION^ADVERB^NN': 70, 'REL^COP': 71, 'REL^BN': 72, 'PREPOSITION^yyQUOT^NN': 73, 'CONJ^IN^PRP': 74, 'CONJ^REL^NNT': 75, 'CONJ^PREPOSITION^NN': 76, 'yyCLN': 77, 'yyQM': 78, 'ZVL^PREPOSITION^NNT': 79, 'REL^MD': 80, 'CONJ^PREPOSITION^CC': 81, 'REL^AT': 82, 'ZVL^DEF^NNT': 83, 'PREPOSITIONIN^PREPOSITION^NN': 84, 'PREPOSITION^DEF^NN': 85, 'REL^DEF^JJ': 86, 'DT': 87, 'VB': 88, 'CONJ': 89, 'IN^IN': 90, 'PREPOSITION^IN': 91, 'CONJ^CDT': 92, 'PREPOSITION^REL^VB': 93, 'CONJ^REL^VB': 94, 'PREPOSITION^VB': 95, 'REL^CC': 96, 'ZVL^PREPOSITION^DEF^NN': 97, 'PREPOSITION^PREPOSITION^DEF^NN': 98, 'CONJ^COP': 99, 'REL^yyQUOT^COP': 100, 'IN^NCD': 101, 'CONJ^yyQUOT^NNP': 102, 'CONJ^PREPOSITION^DEF^NNP': 103, 'CONJ^DEF^CD': 104, 'PREPOSITION^PREPOSITION^DEF^DEF': 105, 'PREPOSITION^REL^COP': 106, 'AT^S_PRN': 107, 'CONJ^IN^S_PRN': 108, 'DEF^yyQUOT^JJ': 109, 'DEF^MD': 110, 'CONJ^QW': 111, 'yyDOT': 112, 'CONJ^RB': 113, 'CC': 114, 'REL^PREPOSITION^DEF^PRP': 115, 'REL^IN^PRP': 116, 'PREPOSITION^DEF^CDT': 117, 'IN^CDT': 118, 'PREPOSITION^DEF^RB': 119, 'REL^PREPOSITION^RB': 120, 'IN^RB': 121, 'CONJ^DT': 122, 'CONJ^TEMP^VB': 123, 'REL^yyQUOT^MD': 124, 'CONJ^PREPOSITION^PRP': 125, 'PREPOSITION^DEF^JJ': 126, 'NN^yyDOT': 127, 'REL^yyQUOT^PRP': 128, 'CONJ^PREPOSITION^NNP': 129, 'MD': 130, 'CONJ^PREPOSITION^DEF^CD': 131, 'CONJ^JJ': 132, 'CONJ^PREPOSITION^BN': 133, 'yySCLN': 134, 'DEF^JJ': 135, 'PREPOSITION^PREPOSITION^NN': 136, 'PREPOSITION^IN^DEF^PRP': 137, 'CONJ^DEF^NN': 138, 'CONJ^DEF^yyQUOT^NN': 139, 'REL^DEF^NN': 140, 'ZVL^PREPOSITION^NN': 141, 'REL^ADVERB^CD': 142, 'PREPOSITION^CD': 143, 'REL^yyQUOT^BN': 144, 'INTJ': 145, 'REL^PRP': 146, 'VB^AT^PRP': 147, 'REL^PREPOSITION^yyQUOT^NNP': 148, 'CC^ZVL^DEF^NN': 149, 'RB': 150, 'CONJ^PREPOSITION^RB': 151, 'yyDASH': 152, 'CONJ^REL^IN': 153, 'TEMP^DEF^BN': 154, 'DEF^CD': 155, 'ZVL^CD': 156, 'CONJ^PREPOSITION^CD': 157, 'DEF^COP': 158, 'PRP': 159, 'CONJ^AT': 160, 'VB^AT^S_PRN': 161, 'NNP': 162, 'CONJ^PREPOSITION^BNT': 163, 'P': 164, 'CONJ^BN': 165, 'PREPOSITION^yyQUOT^NNT': 166, 'yyEXCL': 167, 'JJT': 168, 'REL^VB': 169, 'CONJ^PREPOSITION^yyQUOT^CDT': 170, 'TEMP^DEF^NN': 171, 'REL^DEF^NNP': 172, 'CONJ^VB': 173, 'PREPOSITION^PRP': 174, 'DEF^BN^AT^PRP': 175, 'yyELPS': 176, 'P^NN': 177, 'REL^PREPOSITION^DT': 178, 'CONJ^REL^DEF^NN': 179, 'IN^S_PRN': 180, 'NCD': 181, 'PREPOSITION^DT': 182, 'DEF^PREPOSITION^NNT': 183, 'REL^CDT': 184, 'DEF': 185, 'REL^JJT': 186, 'TEMP^IN': 187, 'yyQUOT': 188, 'CONJ^IN^NNT': 189, 'QW': 190, 'POS^S_PRN': 191, 'CONJ^NN': 192, 'CONJ^REL^EX': 193, 'CONJ^PREPOSITION^P': 194, 'PREPOSITION^BN': 195, 'ZVL^DEF^NNP': 196, 'PREPOSITION^NNT': 197, 'ZVL^JJT': 198, 'IN^NNP': 199, 'REL^PREPOSITION^BN': 200, 'CONJ^yyQUOT^DEF^NN': 201, 'PREPOSITION^DEF^P': 202, 'TEMP^COP': 203, 'REL^AT^S_PRN': 204, 'PREPOSITION^yyQUOT^NNP': 205, 'REL^P': 206, 'PREPOSITION^yyQUOT^PREPOSITION^NNT': 207, 'ZVL^ZVL': 208, 'REL^PREPOSITION^NNP': 209, 'TEMP^PREPOSITION^NNP': 210, 'NEG': 211, 'CONJ^PREPOSITION^DTT': 212, 'REL^yyQUOT^PREPOSITION^NN': 213, 'REL^DEF^CD': 214, 'CONJ^NNT': 215, 'PREPOSITION^RB^S_PRN': 216, 'DEF^NNP': 217, 'CONJ^POS': 218, 'REL^yyQUOT^DEF^NN': 219, 'PREPOSITION^ PREPOSITION^DEF^NN': 220, 'DEF^NCD': 221, 'REL^NNT': 222, 'PREPOSITION^DEF^NNT': 223, 'IN^REL^NNT': 224, 'ZVL^RB': 225, 'BN': 226, 'PREPOSITION^NN': 227, 'REL^PREPOSITION^NN': 228, 'TEMP^PREPOSITION^NN': 229, 'ZVL^NNT': 230, 'TEMP^NN': 231, 'PREPOSITION^BNT': 232, 'CDT': 233, 'DEF^yyQUOT^NNP': 234, 'PREPOSITION^yyQUOT^PREPOSITION^DEF^NN': 235, 'PREPOSITION^IN^S_PRN': 236, 'ZVL^DEF^NN': 237, 'CONJ^EX': 238, 'DEF^DEF^NN': 239, 'CONJ^IN^JJT': 240, 'NNT': 241, 'DTT': 242, 'IN^yyQUOT^VB': 243, 'REL^RB': 244, 'EX': 245, 'CONJ^PREPOSITION^QW': 246, 'ADVERB^NCD': 247, 'TEMP^BN': 248, 'CONJ^JJT': 249, 'CONJ^PREPOSITION^yyQUOT^NNP': 250, 'IN^DEF^NN': 251, 'CONJ^CD': 252, 'REL^CD': 253, 'DEF^NNT': 254, 'CONJ^BNT': 255, 'REL^PREPOSITION^NNT': 256, 'PREPOSITION^POS': 257, 'ZVL': 258, 'PREPOSITION^NEG': 259, 'PREPOSITION^yyQUOT^BN': 260, 'REL^QW': 261, 'CONJ^DEF^DTT': 262, 'REL^VB^AT^PRP': 263, 'CONJ^DEF^P': 264, 'ZVL^IN': 265, 'PREPOSITIONIN^NN': 266, 'REL^PREPOSITION^DEF^NN': 267, 'PREPOSITION^DEF^PRP': 268, 'PREPOSITION^NCD': 269, 'IN^JJT': 270, 'yyCM': 271, 'REL^IN': 272, 'REL^IN^S_PRN': 273, 'CONJ^DEF^MD': 274, 'REL^yyQUOT^RB': 275, 'REL^yyQUOT^VB': 276, 'PREPOSITION^PREPOSITION^NNT': 277, 'CONJ^yyQUOT^VB': 278, 'CONJ^REL^NN': 279, 'DEF^yyQUOT^NN': 280, 'REL^NN': 281, 'PREPOSITION': 282, 'COP': 283, 'REL^EX': 284, 'AT^PRP': 285, 'TEMP^PREPOSITION^PRP': 286, 'CONJ^PREPOSITION^DEF^NN': 287, 'IN^RB^CD': 288, 'REL^NNP': 289, 'CONJ^BN^AT^PRP': 290, 'ADVERB^NN': 291, 'CONJ^TEMP^BN': 292, 'CONJ^PREPOSITION^NNT': 293, 'CONJ^DEF^BN': 294, 'PREPOSITION^RB': 295, 'DEF^BN': 296, 'REL^yyQUOT^NN': 297, 'PREPOSITION^DEF^NNP': 298, 'CONJ^P': 299, 'ZVL^NNP': 300, 'IN^DTT': 301, 'ADVERB^CD': 302, 'CONJ^PREPOSITION^JJ': 303, 'ZVL^COP': 304, 'CONJ^PREPOSITION^DEF^JJ': 305, 'CONJ^PREPOSITION^DEF^PRP': 306, 'PREPOSITION^P': 307, 'RB^S_PRN': 308, 'PREPOSITION^QW': 309, 'ZVL^NN': 310, 'DEF^PRP': 311, 'PREPOSITION^DTT': 312, 'yyRRB': 313, 'TEMP^DEF^CD': 314}\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "data = train_df\n",
    "tag_vals = list(set(data[\"upostag\"].values))\n",
    "tags = ['PAD'] + tag_vals\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "print(tag2idx)\n",
    "# print(idx2tag)\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences_and_labels(tokenized_texts, labels):\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen = MAX_LEN, dtype = \"float32\", truncating = \"post\", padding = \"post\", value = tag2idx['PAD'])\n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "                         maxlen = MAX_LEN, value = tag2idx['PAD'], padding = \"post\",\n",
    "                        dtype = \"float32\", truncating = \"post\")\n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "    return input_ids, tags, attention_masks\n",
    "\n",
    "input_ids, tags, attention_masks = pad_sentences_and_labels(train_tokenized_texts, train_tokenized_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
    "tr_tags = torch.tensor(tags, dtype=torch.long)\n",
    "tr_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|▋         | 1/15 [00:58<13:45, 58.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 2.2720749048810256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  13%|█▎        | 2/15 [02:02<13:05, 60.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.821627008679666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|██        | 3/15 [03:12<12:36, 63.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.5413947211284387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  27%|██▋       | 4/15 [04:27<12:16, 66.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.39871315207136304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 5/15 [05:46<11:42, 70.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.31264687878520864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|████      | 6/15 [07:04<10:54, 72.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.25154486896568223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  47%|████▋     | 7/15 [08:23<09:56, 74.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.20845407497529922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  53%|█████▎    | 8/15 [09:42<08:50, 75.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.17870022454544118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|██████    | 9/15 [11:02<07:43, 77.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.15089197540165564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 10/15 [12:25<06:34, 78.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.13223850584932065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  73%|███████▎  | 11/15 [13:46<05:17, 79.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.11638171909573047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████  | 12/15 [15:05<03:58, 79.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.10619803432277158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  87%|████████▋ | 13/15 [16:26<02:39, 79.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.09557023781694864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  93%|█████████▎| 14/15 [17:48<01:20, 80.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.09014363510926303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 15/15 [19:06<00:00, 76.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.08390635945589135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased',\n",
    "                                                   num_labels=len(tag2idx),\n",
    "                                                   output_attentions = False,\n",
    "                                                   output_hidden_states = False)\n",
    "model.cuda()\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "#     print (pred_flat, labels_flat)\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "epochs = 15\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item() \n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function receives a sentence with its labels, and the tokenized sentence and labels\n",
    "def aggr_toks_labels_tags(orig_words, orig_labels, tok_wordps, tok_labels, predicted_tags):\n",
    "    \n",
    "    joint_tokens = []\n",
    "    joint_labels = []\n",
    "    joint_predicted = []\n",
    "#     joint_test = []\n",
    "    \n",
    "    for word in orig_words:\n",
    "        aggregated_tokenized = \"\"\n",
    "        aggregated_label = \"\"\n",
    "        aggregated_predicted = \"\"\n",
    "        aggregated_test = \"\"\n",
    "        \n",
    "        while aggregated_tokenized != word:\n",
    "#             print(len(tok_sent))\n",
    "            tmpTok = tok_wordps.pop(0)\n",
    "#             print(tmpTok)\n",
    "#             print(joint_tokens)\n",
    "            if tmpTok.startswith(\"##\"):\n",
    "                tmpTok = tmpTok[2:]\n",
    "                \n",
    "            tmpLab = tok_labels.pop(0)\n",
    "#             if aggregated_label == \"\":\n",
    "            aggregated_label += '^'\n",
    "            aggregated_label += tmpLab\n",
    "\n",
    "                \n",
    "            tmpPred = predicted_tags.pop(0)\n",
    "#             print(tmpPred)\n",
    "\n",
    "            aggregated_predicted += '^'\n",
    "            aggregated_predicted += tmpPred\n",
    "#             if aggregated_predicted == \"\":\n",
    "#                 aggregated_predicted = tmpPred\n",
    "                \n",
    "#             tmpTest = test_tags.pop(0)\n",
    "#             if aggregated_test == \"\":\n",
    "#                 aggregated_test = tmpTest\n",
    "                \n",
    "            aggregated_tokenized += tmpTok\n",
    "#             print(aggregated_tokenized)\n",
    "            \n",
    "        joint_tokens.append(aggregated_tokenized)\n",
    "        joint_labels.append(aggregated_label)\n",
    "        joint_predicted.append(aggregated_predicted)\n",
    "#         joint_test.append(aggregated_test)\n",
    "        \n",
    "    assert len(joint_tokens) == len(orig_words)\n",
    "    assert len(joint_tokens) == len(joint_predicted)\n",
    "    return joint_tokens, joint_labels, joint_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def delete_pads_from_preds(predicted_tags, test_tags):\n",
    "    clean_predicted = []\n",
    "    clean_test = []\n",
    "    \n",
    "    for ix in range(0, len(test_tags)):\n",
    "        if test_tags[ix] != 'PAD':\n",
    "            clean_predicted.append(predicted_tags[ix])\n",
    "            clean_test.append(test_tags[ix])\n",
    "            \n",
    "    return clean_predicted, clean_test\n",
    "    \n",
    "def calculate_accuracy(df):\n",
    "    numOfCorrectPredictions = 0\n",
    "    for index in df.index:\n",
    "        orig_pos = df.at[index, 'orig_label']\n",
    "        pred_pos = df.at[index, 'predicted_tag']\n",
    "        if orig_pos == pred_pos:\n",
    "            numOfCorrectPredictions += 1\n",
    "    return numOfCorrectPredictions/len(df)\n",
    "                \n",
    "def test_model(sentence, labels, tok_sent, tok_labels, corres_tokens, sent_id):\n",
    "    input_ids, tags, attention_masks = pad_sentences_and_labels([tok_sent], [tok_labels])\n",
    "\n",
    "    val_inputs = torch.tensor(input_ids, dtype=torch.long)\n",
    "    val_tags = torch.tensor(tags, dtype=torch.long)\n",
    "    val_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    counter = 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.append([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        \n",
    "        true_labels.append(label_ids)\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    pred_tags = [idx2tag[p_ii] for p in predictions for p_i in p for p_ii in p_i]\n",
    "    joint_tokenized, joint_labels, preds = aggr_toks_labels_tags(sentence, labels, tok_sent, tok_labels, \n",
    "                                                                        pred_tags)\n",
    "    \n",
    "    tmp = {'word': sentence, 'orig_label': labels, 'predicted_tag': preds, \n",
    "           'corresToken': corres_tokens, 'sent_id': sent_id}\n",
    "    tmp_df = pd.DataFrame(data=tmp)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1): = 0.0\n"
     ]
    }
   ],
   "source": [
    "full_dev_df = pd.DataFrame()\n",
    "dev_tokenized_texts, dev_tokenized_labels = tokenize(dev_sentences, dev_labels)\n",
    "for sent, label, tok_sent, tok_label, corresTokens, sent_id in zip(dev_sentences, dev_labels, dev_tokenized_texts, \n",
    "                                                                   dev_tokenized_labels, dev_corresTokens, \n",
    "                                                                   dev_sent_ids):\n",
    "    eval_df = test_model(sent, label, tok_sent, tok_label, corresTokens, sent_id)\n",
    "    full_dev_df = full_dev_df.append(eval_df, ignore_index=True, sort=False)\n",
    "\n",
    "# full_df\n",
    "f1_accuracy = calculate_accuracy(full_dev_df)\n",
    "print(\"Accuracy (F1): = {}\".format(f1_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>orig_label</th>\n",
       "      <th>predicted_tag</th>\n",
       "      <th>corresToken</th>\n",
       "      <th>sent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>עשרות</td>\n",
       "      <td>CDT</td>\n",
       "      <td>^CD</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>אנשים</td>\n",
       "      <td>NN</td>\n",
       "      <td>^NN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>מגיעים</td>\n",
       "      <td>BN</td>\n",
       "      <td>^BN^BN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>מתאילנד</td>\n",
       "      <td>PREPOSITION^NNP</td>\n",
       "      <td>^PREPOSITION^NNP^PREPOSITION^NNP^PREPOSITION^NNP</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>לישראל</td>\n",
       "      <td>PREPOSITION^NNP</td>\n",
       "      <td>^PREPOSITION^NNP</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word       orig_label                                     predicted_tag  \\\n",
       "0    עשרות              CDT                                               ^CD   \n",
       "1    אנשים               NN                                               ^NN   \n",
       "2   מגיעים               BN                                            ^BN^BN   \n",
       "3  מתאילנד  PREPOSITION^NNP  ^PREPOSITION^NNP^PREPOSITION^NNP^PREPOSITION^NNP   \n",
       "4   לישראל  PREPOSITION^NNP                                  ^PREPOSITION^NNP   \n",
       "\n",
       "   corresToken  sent_id  \n",
       "0            1        1  \n",
       "1            2        1  \n",
       "2            3        1  \n",
       "3            4        1  \n",
       "4            5        1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (F1): = 0.0\n"
     ]
    }
   ],
   "source": [
    "full_test_df = pd.DataFrame()\n",
    "test_tokenized_texts, test_tokenized_labels = tokenize(test_sentences, test_labels)\n",
    "for sent, label, tok_sent, tok_label, corresTokens, sent_id in zip(test_sentences, test_labels, test_tokenized_texts, \n",
    "                                                                   test_tokenized_labels, test_corresTokens, \n",
    "                                                                   test_sent_ids):\n",
    "    eval_df = test_model(sent, label, tok_sent, tok_label, corresTokens, sent_id)\n",
    "    full_test_df = full_test_df.append(eval_df, ignore_index=True, sort=False)\n",
    "\n",
    "# full_df\n",
    "f1_accuracy = calculate_accuracy(full_test_df)\n",
    "print(\"Accuracy (F1): = {}\".format(f1_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_dev_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluating on gold-dev/gold-test only - regrouping the tokens to words\n",
    "dev_predicted = full_dev_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.predicted_tag)).reset_index()\n",
    "dev_original = full_dev_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.orig_label)).reset_index()\n",
    "dev_combined = pd.merge(dev_original, dev_predicted, on=['sent_id', 'corresToken'])\n",
    "dev_combined.rename(columns = {\"0_x\": \"orig_label\", \"0_y\":\"predicted_tag\"}, inplace = True)\n",
    "\n",
    "test_predicted = full_test_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.predicted_tag)).reset_index()\n",
    "test_original = full_test_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.orig_label)).reset_index()\n",
    "test_combined = pd.merge(test_original, test_predicted, on=['sent_id', 'corresToken'])\n",
    "test_combined.rename(columns = {\"0_x\": \"orig_label\", \"0_y\":\"predicted_tag\"}, inplace = True)\n",
    "# word_acc_test = full_test_df.groupby(['sent_id', 'corresToken']).apply(lambda x: '^'.join(x.predicted_tag)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_combined.to_csv('ftRaw_evalGoldDev_whole_pos_shuffle.csv')\n",
    "test_combined.to_csv('ftRaw_evalGoldTest_whole_pos_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import unique_everseen\n",
    "\n",
    "def unique_vals_to_list(df):\n",
    "    for index in df.index:\n",
    "        joint_pred = df.at[index, 'predicted_tag']\n",
    "        joint_orig = df.at[index, 'orig_label']\n",
    "        \n",
    "        predicted_tag_list = joint_pred.split('^')\n",
    "        predicted_tag_list_no_empty = list(filter(None, predicted_tag_list))\n",
    "        original_tag_list = joint_orig.split('^')\n",
    "        original_tag_list_no_empty = list(filter(None, original_tag_list))\n",
    "\n",
    "        \n",
    "        df.at[index, 'predicted_tag'] = list(unique_everseen(predicted_tag_list_no_empty))\n",
    "        df.at[index, 'orig_label'] = list(unique_everseen(original_tag_list_no_empty))\n",
    "        \n",
    "        \n",
    "unique_vals_to_list(full_dev_df)\n",
    "unique_vals_to_list(full_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_combined.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match_accuracy(df):\n",
    "    exact_matches = 0\n",
    "    for index in df.index:\n",
    "        if df.at[index, 'orig_label'] == df.at[index, 'predicted_tag']:\n",
    "            exact_matches += 1\n",
    "            \n",
    "    return exact_matches\n",
    "\n",
    "print(\"DEV - Exact Match Accuracy = {0:.2f}%\".format(exact_match_accuracy(dev_combined)/len(dev_combined) * 100))\n",
    "print(\"TEST - Exact Match Accuracy = {0:.2f}%\".format(exact_match_accuracy(test_combined)/len(test_combined) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existence_accuracy(df):\n",
    "    # correct tag = appeared in predicted and in gold\n",
    "    total_orig_num_of_labels = 0\n",
    "    total_predicted_num_of_labels = 0\n",
    "    total_num_of_correct_tags = 0\n",
    "    \n",
    "    for index in df.index:\n",
    "        orig_list = df.at[index, 'orig_label']\n",
    "        predicted_list = df.at[index, 'predicted_tag']\n",
    "        total_orig_num_of_labels += len(orig_list)\n",
    "        total_predicted_num_of_labels += len(predicted_list)\n",
    "        total_num_of_correct_tags += len(set(orig_list).intersection(set(predicted_list)))\n",
    "        \n",
    "    precision = total_num_of_correct_tags / total_predicted_num_of_labels * 100\n",
    "    recall = total_num_of_correct_tags / total_orig_num_of_labels * 100\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    print(\"Precision: {0:.2f}%\".format(precision))\n",
    "    print(\"Recall: {0:.2f}%\".format(recall))\n",
    "    print(\"F1: {0:.2f}%\".format(f1))\n",
    "    \n",
    "print(\"DEV:\")\n",
    "existence_accuracy(dev_combined)\n",
    "print(\"TEST:\")\n",
    "existence_accuracy(test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morphbert",
   "language": "python",
   "name": "morphbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
